# アンサンブル学習について
アンサンブル学習の基本概念は、「複数の弱学習機を組み合わせることで、精度の高い予測が可能になる」というものである。
$$  \bm{x_i} \in X (i = 1,2,...,N): 入力データ $$
$$ y_i : 実際の真の値 $$
$$ \hat{y_i} =  f_m(x_i)$$
$$  \hat{real \ y_i} =  w_1f_1(x_i) +  w_2f_2(x_i) ... +  w_mf_m(x_i) $$

## バギング
ブートストラップ法を用いた復元抽出によってデータに多様性を持たし、複数の予測機を作成する。

## ブースティング
予測機の学習を順次的に行う。不正解したデータをより重点的に学習していき、複数の予測機の誤差が小さいほど、その識別器の結果を反映させる。
- 特徴
  - 並列処理できないため、学習の時間がかかる。
  - 過学習を起こしやすいため、弱学習器には小さな木が用いられる。

### AdaBoosting
[はじパタのAdaBoosting](https://ysk24ok.github.io/2016/09/27/hajipata-boosting.html)
- 学習順序
  1. 重みを$w_i^1 = 1/ N$で初期化する。
      > wは学習機のパラメータでなく、各サンプルの予測結果をどれだけ反映させるかという重み。
  2. モデルFを学習させる。
  3. モデルf_mの誤差$E_m$が最小になるように学習をする。
    $$E_m = \frac{\sum_{i=1}^{N} w_i^m I(f_m(\bm{x_i}) \neq y_i) }{\sum_{i=1}^{N} w_i^m} $$
  4. 弱識別器$f_m$に対する重み$\alpha_m$を計算する。
    $$\alpha_m = ln(\frac{1-E_m}{E_m})$$
  5. 重み$w_i^m$を更新して$w_i^{m+1}$を計算する。(不正解データは、元の重みに$exp(\alpha^m) (> 0)$ の積とする。)
    $$w_i^{m+1} = w_i exp(\alpha_m  I(f_m(\bm{x_i}) \neq y_i) )$$
  6. 2~5を繰り返して全ての$\alpha_m$を求める。
  7. 入力に対する結果を求める。
    $$F_M(\bm{x}) = sign(\sum_{m=1}^{M} a_m f_m(\bm{x}) ))$$

## スタッキング

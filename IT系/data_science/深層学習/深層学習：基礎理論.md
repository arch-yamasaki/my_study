## 深層学習の基本
### 記号
- W (重み・パラメータの集合)
- X (入力データ)
- Y (出力データ)
- Labels (真の値・ラベル)
- Loss (Loss関数)

### 作業
- Xの前処理
- Model(Y = WX)の作成。Wは非常に多数のパラメータ。
- モデル学習
  - Forwardの計算を行う( $Y = Model(X)$ )
  - 誤差Eを求める。( $E = Loss(Y, Labels)$)
  - 各$w$(スカラー)に対する勾配$\frac{\partial E}{\partial w}$(= Grad)を求める。
  - 各$w$計算したGtadとoptimizerに基づいて更新。
  - 繰り返す。
  
> ミニバッチの場合、BatchSize=Nとすると、
Eを $E = \frac{1}{N}\Sigma_i^N {Loss(X_i, Labels_i ; W)}$(バッチサイズ中のデータの誤差の平均)として、
勾配$\frac{\partial E}{\partial w}$ (=Grad)を求める。
(Grad求める際に、X含む途中の入力が必要となるが、それも多分平均値を用いてGradを計算してそう。)




## 順伝播ネットワーク
- 活性化関数

## 勾配効果法
- 確率的勾配効果法

## 誤差逆伝播法

- 勾配焼失・爆発のための工夫
  1. 活性化関数の変更(ReLUなど)
  2. ネットワークの重みの初期値を事前学習
  3. 学習係数を下げる
  4. ネットワークの自由度を制約する。(DropOutなど)
  5. Batch Normalization(BN)
      - 層毎の出力を毎回標準化もしくは白色化(whitniga)する。
      - DCGANで一番効果あったのがAdamとBN。BNいれれば他の問題も大きく変わってくる、とまで言わしめる手法。
  6. その他モデルにより様々な試みがされている。



## 今回
<a name-anker></a>

---
# RNN
ざっくりの話は以下URLより
[DeepAge:RNN：時系列データを扱うRecurrent Neural Networksとは](https://deepage.net/deep_learning/2017/05/23/recurrent-neural-networks.html)
RNN何でも出来過ぎ、強すぎ、って記事(未読)
[リカレントニューラルネットワークの理不尽な効力（翻訳）](https://qiita.com/KojiOhki/items/397f157342e0def06a9b)

## シンプルなモデル
RNNにおいて
入力$x_t$、出力$y_t$とすると

## LSTM

## GRU

## Bi-directional RNN

## Quasi-Recurrent Neural Network(QRNN)

## seq2seq
入力が$(x_1,x_2, ... , xm)$
出力が$(y_1,y_2, ... , yn)$
とすると、
![スクリーンショット 2018-08-09 22.14.08.png](resources/4E1226B4FCF87CDE1B6BDCAD1ED43068.png =592x357)
参考URL
[DeepLearning における会話モデル](https://qiita.com/halhorn/items/646d323ac457715866d4)
[Seq2Seq まとめ](http://higepon.hatenablog.com/entry/20171210/1512887715)